from fastapi import FastAPI, Request
from pydantic import BaseModel
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from langchain.tools import Tool
from langchain_core.language_models import BaseLLM
from langchain_core.language_models.llms import LLMResult
from langchain_core.outputs import Generation
import requests
import json
import os
from client import MCPToolAdapter
import asyncio
from typing import List, Optional, Tuple
from generate_token import generate_token
import logging
import sys
import time

# Remove all handlers associated with the root logger object.
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)

app = FastAPI()

class Query(BaseModel):
    query: str

# Configuration
OPENAI_API_URL = "https://bgt-app-bgpt.enbgai.prd.prd.bfsaws.net/v1/chat/completions"
REQUEST_TIMEOUT = 120  # Increased timeout to 120 seconds
TOKEN_EXPIRY = 3600  # 1 hour token expiry

# Global state
mcp_adapter = None
api_token = None
token_generation_time = 0

async def get_mcp_adapter():
    global mcp_adapter
    if mcp_adapter is None:
        server_params = {"command": "python", "args": ["server.py"], "env": None}
        mcp_adapter = MCPToolAdapter(server_params)
        await mcp_adapter.connect()
    return mcp_adapter

def get_api_token() -> str:
    global api_token, token_generation_time
    current_time = time.time()
    
    # Regenerate token if expired or not exists
    if api_token is None or (current_time - token_generation_time) > TOKEN_EXPIRY:
        logging.info("Generating new API token")
        api_token = generate_token()
        token_generation_time = current_time
    return api_token

async def get_mcp_tools() -> List[dict]:
    tools = []
    try:
        adapter = await get_mcp_adapter()
        response = await adapter.list_tools()
        
        logging.debug(f"[MCP TOOLS] Raw response: {response}")
        
        # Handle different response formats from MCP
        tool_list = []
        if hasattr(response, 'tools'):
            tool_list = response.tools
        elif isinstance(response, list):
            tool_list = response
        elif isinstance(response, dict) and 'tools' in response:
            tool_list = response['tools']
        else:
            logging.warning(f"[MCP TOOLS] Unexpected response format: {type(response)}")
            return tools
        
        for tool in tool_list:
            try:
                if isinstance(tool, dict):
                    name = tool.get('name')
                    description = tool.get('description', '')
                    input_schema = tool.get('inputSchema', {})
                else:
                    name = getattr(tool, 'name', None)
                    description = getattr(tool, 'description', '')
                    input_schema = getattr(tool, 'inputSchema', {})
                
                if name:
                    full_description = description
                    if input_schema and isinstance(input_schema, dict):
                        properties = input_schema.get('properties', {})
                        if properties:
                            params_info = ", ".join(properties.keys())
                            full_description += f" (Parameters: {params_info})"
                    
                    tools.append({
                        "name": name,
                        "description": full_description,
                        "input_schema": input_schema
                    })
            except Exception as e:
                logging.error(f"[MCP TOOLS] Error processing tool {tool}: {e}")
        
        logging.info(f"[MCP TOOLS] Successfully loaded {len(tools)} tools")
    except Exception as e:
        logging.error(f"[MCP TOOLS] Error getting tools: {e}")
    
    return tools

class CustomOpenAILLM(BaseLLM):
    @property
    def _llm_type(self) -> str:
        return "custom-openai-compatible"

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager=None,
        **kwargs
    ) -> LLMResult:
        generations = []
        headers = {
            "Content-Type": "application/json",
            "applicationType": "BRProduct",
            "Authorization": f"Bearer {get_api_token()}"
        }
        
        for prompt in prompts:
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": "o1",
                "max_tokens": 5000,
            }
            
            try:
                response = requests.post(
                    OPENAI_API_URL, 
                    json=data, 
                    headers=headers,
                    timeout=REQUEST_TIMEOUT
                )
                
                if response.status_code == 200:
                    response_data = response.json()
                    answer = response_data["choices"][0]["message"]["content"]
                    generations.append([Generation(text=answer)])
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text}"
                    logging.error(f"API Error: {error_msg}")
                    raise Exception(f"Error in Processing: {error_msg}")
                    
            except requests.exceptions.Timeout:
                error_msg = f"Request timed out after {REQUEST_TIMEOUT} seconds"
                logging.error(error_msg)
                raise Exception(error_msg)
            except requests.exceptions.RequestException as e:
                logging.error(f"Request Exception: {e}")
                raise Exception(f"Network Error: {str(e)}")
            except Exception as e:
                logging.error(f"General Exception: {e}")
                raise
                
        return LLMResult(generations=generations)

async def execute_mcp_tool(tool_name: str, parameters: dict) -> str:
    try:
        adapter = await get_mcp_adapter()
        result = await adapter.execute_tool(tool_name, parameters)
        return str(result)
    except asyncio.TimeoutError:
        error_msg = f"Tool execution timed out for {tool_name}"
        logging.error(error_msg)
        return error_msg
    except Exception as e:
        error_msg = f"Error executing tool {tool_name}: {str(e)}"
        logging.error(error_msg)
        return error_msg

async def setup_agent() -> Tuple[Optional[AgentExecutor], str, str]:
    mcp_tools_list = await get_mcp_tools()
    
    if not mcp_tools_list:
        logging.warning("No MCP tools available!")
        return None, "", ""

    def make_tool_func(tool_name: str):
        async def async_tool_func(params_input: str) -> str:
            try:
                params = json.loads(params_input) if isinstance(params_input, str) else params_input
                return await execute_mcp_tool(tool_name, params)
            except Exception as e:
                return f"Error executing tool {tool_name}: {str(e)}"
        
        return async_tool_func

    tools = []
    for tool in mcp_tools_list:
        try:
            tools.append(Tool(
                name=tool["name"],
                func=make_tool_func(tool["name"]),
                description=tool["description"],
                coroutine=make_tool_func(tool["name"])
            ))
        except Exception as e:
            logging.error(f"Error creating tool {tool['name']}: {e}")

    if not tools:
        logging.error("No valid tools created!")
        return None, "", ""

    tool_names = ", ".join([tool.name for tool in tools])
    tools_str = "\n".join([f"{tool.name}: {tool.description}" for tool in tools])

    llm = CustomOpenAILLM()
    
    prompt = PromptTemplate(
        template=(
            "Answer questions using these tools:\n\n{tools}\n\n"
            "Format:\n"
            "Question: input question\n"
            "Thought: think about action\n"
            "Action: tool name from [{tool_names}]\n"
            "Action Input: valid JSON input\n"
            "Observation: tool result\n"
            "... (repeat if needed)\n"
            "Thought: final answer\n"
            "Final Answer: exact tool output\n\n"
            "RULES:\n"
            "1. MUST use tools to answer\n"
            "2. Return EXACT tool output\n"
            "3. Action Input MUST be valid JSON\n"
            "4. If error, return it exactly\n"
            "5. If no tool, say 'No suitable tool'\n\n"
            "Begin!\n\n"
            "Question: {input}\n"
            "Thought:{agent_scratchpad}"
        ),
        input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
    )
    
    agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)
    agent_executor = AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=True,
        handle_parsing_errors=True,
        return_intermediate_steps=True,
        max_iterations=5,
        early_stopping_method="generate"
    )
    
    return agent_executor, tool_names, tools_str

@app.post("/process_query")
async def process_query(query: Query, request: Request):
    try:
        agent_executor, tool_names, tools_str = await setup_agent()
        
        if not agent_executor:
            return {"error": "No tools available"}
        
        result = await agent_executor.ainvoke({
            "input": query.query,
            "tools": tools_str,
            "tool_names": tool_names,
            "agent_scratchpad": ""
        })
        
        if isinstance(result, dict):
            if "output" in result:
                output = result["output"]
                if "intermediate_steps" in result:
                    for step in result["intermediate_steps"]:
                        if isinstance(step, tuple) and len(step) >= 2:
                            tool_output = step[1]
                            if tool_output:
                                output = tool_output
                                break
                return {"result": output}
            return {"result": str(result)}
        
        return {"result": str(result)}
        
    except Exception as e:
        logging.error(f"Error processing query: {e}")
        return {"error": f"Failed to process query: {str(e)}"}

# ... (rest of the endpoints remain the same)
