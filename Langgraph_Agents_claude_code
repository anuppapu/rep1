# ============================================================================
# ENHANCED LANGGRAPH v1.0 â€“ PRODUCTION-READY AGENT WITH AUTO INTENT DETECTION
# ============================================================================

from typing import TypedDict, Optional, Literal, Annotated
from datetime import datetime
import json
import uuid
import hashlib
import time
import concurrent.futures
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import requests

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from langchain_core.messages import (
    HumanMessage,
    AIMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_openai import ChatOpenAI
from generate_token import generate_token


# ============================================================================
# STATE DEFINITION
# ============================================================================

class AgentState(TypedDict):
    messages: Annotated[list, add_messages]

    # Outputs
    requirements_output: Optional[dict]
    testcases_output: Optional[dict]

    # Human approval
    human_approval_required: bool
    human_approved: Optional[bool]
    human_feedback: Optional[str]
    process_terminated: bool

    # Control
    error: Optional[str]
    iteration_count: int
    max_iterations: int
    session_id: str
    timestamp: str
    
    # Document handling
    uploaded_files: list[dict]
    
    # Execution mode
    execution_mode: Optional[str]
    requirements_document: Optional[str]
    testcases_document: Optional[str]
    
    # Progress tracking
    progress: Optional[dict]
    
    # Intent cache
    intent_cache_key: Optional[str]
    
    # Retry tracking
    retry_count: dict


# ============================================================================
# INTENT INFERENCE FUNCTION (NEW)
# ============================================================================

def infer_execution_mode(user_query: str, uploaded_files: list = None) -> str:
    """
    Automatically infer execution mode from user query and uploaded files.
    
    Args:
        user_query: User's input query
        uploaded_files: List of uploaded file dictionaries
    
    Returns:
        execution_mode: One of "requirements_only", "testcases_only", "sequential", "parallel"
    """
    query_lower = user_query.lower()
    file_count = len(uploaded_files) if uploaded_files else 0
    
    # Keywords for different intents
    requirements_keywords = [
        "requirement", "requirements", "req", "reqs", "specification", 
        "spec", "functional", "business rule", "user story", "feature"
    ]
    
    testcases_keywords = [
        "test case", "testcase", "test cases", "testcases", "tc", "tcs",
        "test scenario", "test", "testing", "qa", "quality"
    ]
    
    both_keywords = [
        "both", "all", "everything", "complete", "full", "end to end", 
        "end-to-end", "e2e", "entire"
    ]
    
    parallel_keywords = [
        "parallel", "simultaneously", "at the same time", "together",
        "independent", "separately", "different documents"
    ]
    
    # Count keyword matches
    req_count = sum(1 for kw in requirements_keywords if kw in query_lower)
    tc_count = sum(1 for kw in testcases_keywords if kw in query_lower)
    both_count = sum(1 for kw in both_keywords if kw in query_lower)
    parallel_count = sum(1 for kw in parallel_keywords if kw in query_lower)
    
    print(f"\nðŸ” Intent Detection:")
    print(f"   Query: {user_query}")
    print(f"   Files: {file_count}")
    print(f"   Requirement keywords: {req_count}")
    print(f"   Test case keywords: {tc_count}")
    print(f"   Both keywords: {both_count}")
    print(f"   Parallel keywords: {parallel_count}")
    
    # Decision logic
    
    # Case 1: Parallel execution (2+ files + parallel keywords OR explicit parallel request)
    if (file_count >= 2 and parallel_count > 0) or (both_count > 0 and file_count >= 2):
        print(f"   âžœ Decision: PARALLEL (2+ files, parallel indicators)\n")
        return "parallel"
    
    # Case 2: Both keywords with single file -> Sequential
    if both_count > 0 and file_count >= 1:
        print(f"   âžœ Decision: SEQUENTIAL (both keywords, single file)\n")
        return "sequential"
    
    # Case 3: Requirements only
    if req_count > tc_count and tc_count == 0:
        print(f"   âžœ Decision: REQUIREMENTS_ONLY (requirements keywords dominant)\n")
        return "requirements_only"
    
    # Case 4: Test cases only
    if tc_count > req_count and req_count == 0:
        print(f"   âžœ Decision: TESTCASES_ONLY (test case keywords dominant)\n")
        return "testcases_only"
    
    # Case 5: Both mentioned -> Sequential (if 1 file) or Parallel (if 2+ files)
    if req_count > 0 and tc_count > 0:
        if file_count >= 2:
            print(f"   âžœ Decision: PARALLEL (both mentioned, 2+ files)\n")
            return "parallel"
        else:
            print(f"   âžœ Decision: SEQUENTIAL (both mentioned, 1 file)\n")
            return "sequential"
    
    # Default: Sequential if ambiguous and has files
    if file_count >= 1:
        print(f"   âžœ Decision: SEQUENTIAL (default with files)\n")
        return "sequential"
    
    # Fallback: Requirements only
    print(f"   âžœ Decision: REQUIREMENTS_ONLY (fallback)\n")
    return "requirements_only"


# ============================================================================
# ENHANCED LLM-BASED INTENT ANALYZER (NEW)
# ============================================================================

def llm_infer_execution_mode(user_query: str, uploaded_files: list = None) -> dict:
    """
    Use LLM to infer execution mode with detailed reasoning.
    
    Returns:
        dict with 'execution_mode', 'reasoning', 'confidence'
    """
    api_key = generate_token()
    llm = ChatOpenAI(
        model="gpt-5.2",
        openai_api_base="https://bgt-app-bgpt.enbgai.prd.prd.bfsaws.net/v1/",
        api_key=api_key,
        default_headers={"applicationType": "BRProduct"},
        temperature=0,
        timeout=30,
        max_tokens=500,
    )
    
    file_count = len(uploaded_files) if uploaded_files else 0
    file_info = ""
    if uploaded_files:
        file_info = f"\nUploaded files ({file_count}):\n"
        for i, f in enumerate(uploaded_files, 1):
            file_info += f"  {i}. {f.get('name', 'unknown')} ({f.get('type', 'unknown')})\n"
    
    system_prompt = """You are an intent classification expert for a requirements and test case generation system.

Analyze the user's query and uploaded files to determine the execution mode.

Execution Modes:
1. **requirements_only**: Generate only requirements
2. **testcases_only**: Generate only test cases
3. **sequential**: Generate requirements first, then test cases from same document
4. **parallel**: Generate requirements and test cases from different documents simultaneously

Classification Rules:
- If query mentions ONLY "requirements/specification" â†’ requirements_only
- If query mentions ONLY "test cases/testing" â†’ testcases_only
- If query mentions "both" or "requirements AND test cases" with 1 file â†’ sequential
- If query mentions "both" or "parallel" with 2+ files â†’ parallel
- If 2+ files with different purposes â†’ parallel
- Default with 1 file and ambiguous query â†’ sequential

Respond ONLY with a JSON object:
{
    "execution_mode": "requirements_only|testcases_only|sequential|parallel",
    "reasoning": "brief explanation",
    "confidence": 0.0-1.0
}"""

    user_message = f"""User Query: {user_query}
{file_info}

Determine the execution mode."""

    try:
        response = llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ])
        
        response_text = response.content.strip()
        
        # Remove markdown code blocks if present
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
        
        result = json.loads(response_text)
        
        print(f"\nðŸ¤– LLM Intent Analysis:")
        print(f"   Mode: {result.get('execution_mode')}")
        print(f"   Reasoning: {result.get('reasoning')}")
        print(f"   Confidence: {result.get('confidence', 0):.2f}\n")
        
        return result
        
    except Exception as e:
        print(f"âš ï¸ LLM inference failed: {e}, using rule-based fallback")
        # Fallback to rule-based
        mode = infer_execution_mode(user_query, uploaded_files)
        return {
            "execution_mode": mode,
            "reasoning": "Fallback to rule-based inference",
            "confidence": 0.7
        }


# ============================================================================
# ENHANCED TOOL FUNCTIONS WITH RETRY LOGIC
# ============================================================================

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException)
)
def requirements_creation_tool(document: str, payload_options: dict) -> dict:
    """
    Requirements creation tool with retry logic.
    """
    import mimetypes
    import os
    from io import BytesIO
    
    AXIOS_URL = "https://onboardgpt.broadridge.net:8884"
    API_ENDPOINT = f"{AXIOS_URL}/generate-current-flow"
    
    user_name = "Anup Das"
    user_email = "anup.das@example.com"
    
    files = []
    file_handles = []
    
    if document:
        if isinstance(document, str) and os.path.exists(document):
            mime_type, _ = mimetypes.guess_type(document)
            if mime_type is None:
                mime_type = "application/octet-stream"
            file_handle = open(document, 'rb')
            file_handles.append(file_handle)
            files.append(('files', (os.path.basename(document), file_handle, mime_type)))
            print(f"ðŸ“„ Uploading file: {document} (MIME: {mime_type})")
        else:
            doc_bytes = str(document).encode('utf-8')
            doc_file = BytesIO(doc_bytes)
            file_handles.append(doc_file)
            files.append(('files', ('document.txt', doc_file, 'text/plain')))
            print(f"ðŸ“„ Uploading text content as document.txt")
    
    data = {
        "userName": user_name,
        "userEmail": user_email
    }
    
    try:
        start_time = time.time()
        print(f"ðŸš€ Sending requirements generation request to: {API_ENDPOINT}")
        
        response = requests.post(API_ENDPOINT, files=files, data=data, verify=False, timeout=600)
        response.raise_for_status()
        
        end_time = time.time()
        total_time = end_time - start_time
        minutes = int(total_time // 60)
        seconds = total_time % 60
        print(f"â±ï¸ Requirements generation time: {minutes}:{seconds:06.3f}")
        
        response_data = response.json()
        print("âœ… Requirements generation completed successfully")
        
        for fh in file_handles:
            if hasattr(fh, 'close'):
                fh.close()
        
        return {
            "status": "success",
            "requirements": response_data,
            "metadata": {
                "api_endpoint": API_ENDPOINT,
                "files_uploaded": len(files),
                "processing_time": f"{minutes}:{seconds:06.3f}",
                "user_name": user_name,
                "user_email": user_email
            },
            "raw_response": response_data
        }

    except requests.exceptions.Timeout:
        print(f"âŒ Timeout error in requirements generation")
        for fh in file_handles:
            if hasattr(fh, 'close'):
                fh.close()
        raise
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Error in requirements generation: {e}")
        for fh in file_handles:
            if hasattr(fh, 'close'):
                fh.close()
        raise
                
    except Exception as e:
        print(f"âŒ Unexpected error in requirements generation: {e}")
        for fh in file_handles:
            if hasattr(fh, 'close'):
                fh.close()
        return {
            "status": "error",
            "error": f"Unexpected error: {str(e)}",
            "metadata": {
                "files_attempted": len(files),
                "user_name": user_name,
                "user_email": user_email
            }
        }


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException)
)
def test_case_generation_tool(requirements: dict, options: dict = None) -> dict:
    """
    Test case generation tool with retry logic.
    """
    import mimetypes
    import os
    import re
    from io import BytesIO
    
    options = options or {}
    
    API_URL = "https://testgenaiqa.broadridge.net:5000/generate-test-cases"
    
    name_test_gen = "System Agent"
    email_test_gen = "agent@broadridge.com"
    session_id = f"agent_session_{int(time.time())}"
    
    support_docs = options.get("support_documents", [])
    product = options.get("product", "")
    sub_products = options.get("sub_products", [])
    execution_mode = options.get("execution_mode", "sequential")
    
    file_handles = []
    
    def sanitize_text(text: str) -> str:
        if not text:
            return ""
        sanitized = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
        sanitized = sanitized.replace('\r\n', '\n').replace('\r', '\n')
        return sanitized
    
    is_sequential = False
    if isinstance(requirements, dict):
        if "requirements" in requirements and requirements.get("status") == "success":
            is_sequential = True
        if execution_mode in ["parallel", "independent"]:
            is_sequential = False
        elif execution_mode == "sequential":
            is_sequential = True
    
    print(f"ðŸ“‹ Execution Mode: {'SEQUENTIAL (text-based)' if is_sequential else 'PARALLEL/INDEPENDENT (file-based)'}")
    
    files = []
    business_text = ""
    reference_text = ""
    
    if is_sequential:
        print("ðŸ“ Using text-based approach for sequential execution")
        
        if isinstance(requirements, dict):
            if "requirements" in requirements:
                req_data = requirements.get("requirements", {})
                if isinstance(req_data, dict):
                    try:
                        business_text = json.dumps(req_data, ensure_ascii=False, indent=2)
                    except (TypeError, ValueError):
                        business_text = str(req_data)
                elif isinstance(req_data, str):
                    business_text = req_data
                else:
                    business_text = str(req_data)
            elif "raw_response" in requirements:
                raw = requirements.get("raw_response", {})
                if isinstance(raw, dict):
                    try:
                        business_text = json.dumps(raw, ensure_ascii=False, indent=2)
                    except (TypeError, ValueError):
                        business_text = str(raw)
                else:
                    business_text = str(raw)
            else:
                try:
                    business_text = json.dumps(requirements, ensure_ascii=False, indent=2)
                except (TypeError, ValueError):
                    business_text = str(requirements)
        else:
            business_text = str(requirements)
        
        business_text = sanitize_text(business_text)
        
        if support_docs:
            reference_texts = []
            for doc in support_docs:
                if isinstance(doc, dict):
                    doc_name = doc.get("name", "")
                    doc_content = doc.get("content", "")
                    purpose = doc.get("purpose", "")
                    
                    if isinstance(doc_content, str) and os.path.exists(doc_content):
                        try:
                            with open(doc_content, 'r', encoding='utf-8', errors='ignore') as f:
                                doc_content = f.read()
                        except Exception as e:
                            print(f"âš ï¸ Could not read support doc file {doc_content}: {e}")
                            doc_content = ""
                    
                    doc_content = sanitize_text(str(doc_content))
                    if doc_content:
                        reference_texts.append(f"Document: {doc_name}\nPurpose: {purpose}\nContent: {doc_content}")
                else:
                    reference_texts.append(sanitize_text(str(doc)))
            reference_text = "\n\n---\n\n".join(reference_texts)
        
        reference_text = sanitize_text(reference_text)
        
    else:
        print("ðŸ“ Using file-based approach for parallel/independent execution")
        
        if isinstance(requirements, dict):
            doc_content = requirements.get("document", requirements.get("content", ""))
            doc_name = requirements.get("name", "business_document")
            
            if doc_content:
                if isinstance(doc_content, str) and os.path.exists(doc_content):
                    mime_type, _ = mimetypes.guess_type(doc_content)
                    if mime_type is None:
                        mime_type = "application/octet-stream"
                    file_handle = open(doc_content, 'rb')
                    file_handles.append(file_handle)
                    files.append(('businessDoc', (os.path.basename(doc_content), file_handle, mime_type)))
                    print(f"ðŸ“„ Uploading business document file: {doc_content} (MIME: {mime_type})")
                else:
                    doc_bytes = str(doc_content).encode('utf-8')
                    doc_file = BytesIO(doc_bytes)
                    file_handles.append(doc_file)
                    
                    if doc_name.endswith(('.pdf', '.doc', '.docx', '.txt')):
                        filename = doc_name
                    else:
                        filename = f"{doc_name}.txt"
                    
                    files.append(('businessDoc', (filename, doc_file, 'text/plain')))
                    print(f"ðŸ“„ Uploading text content as: {filename}")
        
        if support_docs:
            for idx, doc in enumerate(support_docs):
                if isinstance(doc, dict):
                    doc_name = doc.get("name", f"support_doc_{idx}")
                    doc_content = doc.get("content", "")
                    
                    if doc_content:
                        if isinstance(doc_content, str) and os.path.exists(doc_content):
                            mime_type, _ = mimetypes.guess_type(doc_content)
                            if mime_type is None:
                                mime_type = "application/octet-stream"
                            file_handle = open(doc_content, 'rb')
                            file_handles.append(file_handle)
                            files.append(('referenceDoc', (os.path.basename(doc_content), file_handle, mime_type)))
                            print(f"ðŸ“„ Uploading support document file: {doc_content} (MIME: {mime_type})")
                        else:
                            doc_bytes = str(doc_content).encode('utf-8')
                            doc_file = BytesIO(doc_bytes)
                            file_handles.append(doc_file)
                            
                            if doc_name.endswith(('.pdf', '.doc', '.docx', '.txt')):
                                filename = doc_name
                            else:
                                filename = f"{doc_name}.txt"
                            
                            files.append(('referenceDoc', (filename, doc_file, 'text/plain')))
                            print(f"ðŸ“„ Uploading support doc content as: {filename}")
    
    data_tuples = [
        ('businessText', business_text),
        ('referenceText', reference_text),
        ('product', product),
        ('nameTestGen', name_test_gen),
        ('emailTestGen', email_test_gen),
        ('sessionId', session_id)
    ]
    
    for sub in sub_products:
        data_tuples.append(('subProduct[]', sub))
    
    try:
        start_time = time.time()
        print(f"ðŸš€ Sending test case generation request to: {API_URL}")
        print(f"   - Files attached: {len(files)}")
        print(f"   - Business text length: {len(business_text)} chars")
        print(f"   - Reference text length: {len(reference_text)} chars")
        
        response = requests.post(API_URL, files=files if files else None, data=data_tuples, verify=False, timeout=600)
        response.raise_for_status()
        
        end_time = time.time()
        total_time = end_time - start_time
        minutes = int(total_time // 60)
        seconds = total_time % 60
        print(f"â±ï¸ Test case generation time: {minutes}:{seconds:06.3f}")
        
        try:
            response_text = response.text
            response_text = response_text.lstrip('\ufeff')
            response_text = sanitize_text(response_text)
            response_data = json.loads(response_text)
        except json.JSONDecodeError as json_err:
            print(f"âš ï¸ JSON parsing error: {json_err}")
            response_text = response.text
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    response_data = json.loads(sanitize_text(json_match.group()))
                except json.JSONDecodeError:
                    response_data = {"raw_response": response_text, "parse_error": str(json_err)}
            else:
                response_data = {"raw_response": response_text, "parse_error": str(json_err)}
        
        print("âœ… Test case generation completed successfully")
        
        for fh in file_handles:
            if hasattr(fh, 'close'):
                fh.close()
        
        return {
            "status": "success",
            "test_cases": response_data,
            "metadata": {
                "api_endpoint": API_URL,
                "execution_mode": "sequential" if is_sequential else "parallel/independent",
                "files_uploaded": len(files),
                "support_documents_used": len(support_docs),
                "processing_time": f"{minutes}:{seconds:06.3f}",
                "product": product,
                "sub_products_count": len(sub_products),
                "session_id": session_id,
                "name_test_gen": name_test_gen,
                "email_test_gen": email_test_gen
            },
            "raw_response": response_data
        }
        
    except requests.exceptions.Timeout:
        print(f"âŒ Timeout error in test case generation")
        for fh in file_handles:
            if hasattr(fh, 'close'):
                fh.close()
        raise
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Error in test case generation: {e}")
        for fh in file_handles:
            if hasattr(fh, 'close'):
                fh.close()
        raise
        
    except Exception as e:
        print(f"âŒ Unexpected error in test case generation: {e}")
        for fh in file_handles:
            if hasattr(fh, 'close'):
                fh.close()
        return {
            "status": "error",
            "error": f"Unexpected error: {str(e)}",
            "metadata": {
                "support_documents_attempted": len(support_docs),
                "session_id": session_id,
                "name_test_gen": name_test_gen,
                "email_test_gen": email_test_gen
            }
        }


# ============================================================================
# ENHANCED AGENT NODES
# ============================================================================

def agent_planner(state: AgentState) -> AgentState:
    """
    Enhanced agent planner with progress tracking.
    """
    execution_mode = state.get("execution_mode", "sequential")
    requirements_output = state.get("requirements_output")
    testcases_output = state.get("testcases_output")
    human_approved = state.get("human_approved")
    human_approval_required = state.get("human_approval_required")
    
    # Update progress
    progress = {
        "requirements_generated": bool(requirements_output),
        "requirements_approved": bool(human_approved),
        "testcases_generated": bool(testcases_output),
        "current_step": None
    }
    
    # TESTCASES ONLY MODE
    if execution_mode == "testcases_only":
        progress["current_step"] = "testcases"
        if not testcases_output:
            return {
                **state,
                "messages": [AIMessage("Generating test cases from document...")],
                "progress": progress
            }
        progress["current_step"] = "complete"
        return {**state, "progress": progress}
    
    # REQUIREMENTS ONLY MODE
    if execution_mode == "requirements_only":
        progress["current_step"] = "requirements"
        if not requirements_output:
            return {
                **state,
                "messages": [AIMessage("Generating requirements...")],
                "progress": progress
            }
        progress["current_step"] = "complete"
        return {**state, "progress": progress}
    
    # PARALLEL MODE
    if execution_mode == "parallel":
        progress["current_step"] = "parallel"
        if not requirements_output and not testcases_output:
            return {
                **state,
                "messages": [AIMessage("Generating requirements and test cases in parallel...")],
                "progress": progress
            }
        progress["current_step"] = "complete"
        return {**state, "progress": progress}
    
    # SEQUENTIAL MODE
    if not requirements_output:
        progress["current_step"] = "requirements"
        return {
            **state,
            "messages": [AIMessage("Generating requirements...")],
            "progress": progress
        }

    if human_approval_required and human_approved is None:
        progress["current_step"] = "approval"
        return {**state, "progress": progress}

    if human_approved and not testcases_output:
        progress["current_step"] = "testcases"
        return {
            **state,
            "messages": [AIMessage("Generating test cases using existing requirements...")],
            "progress": progress
        }
    
    if requirements_output and testcases_output:
        progress["current_step"] = "complete"
        return {
            **state,
            "messages": [AIMessage("Both requirements and test cases have been generated.")],
            "progress": progress
        }

    return {**state, "progress": progress}


def requirements_node(state: AgentState) -> AgentState:
    """
    Enhanced requirements node with retry tracking.
    """
    uploaded_files = state.get("uploaded_files", [])
    retry_count = state.get("retry_count", {})
    
    document = None
    if uploaded_files:
        first_file = uploaded_files[0]
        document = first_file.get("content") or first_file.get("path") or first_file.get("name")
        print(f"ðŸ“„ Using document: {document}")
    else:
        print("âš ï¸ No uploaded files found, using empty document")
        document = ""
    
    try:
        result = requirements_creation_tool(document, {})
        retry_count["requirements"] = 0
        
        return {
            **state,
            "requirements_output": result,
            "human_approval_required": True,
            "messages": [AIMessage("âœ… Requirements generated.")],
            "retry_count": retry_count,
            "error": None
        }
    except Exception as e:
        retry_count["requirements"] = retry_count.get("requirements", 0) + 1
        return {
            **state,
            "error": f"Requirements generation failed after retries: {str(e)}",
            "messages": [AIMessage(f"âŒ Error: {str(e)}")],
            "retry_count": retry_count
        }


def testcases_node(state: AgentState) -> AgentState:
    """
    Enhanced test cases node with retry tracking.
    """
    execution_mode = state.get("execution_mode", "sequential")
    requirements = state.get("requirements_output")
    testcases_doc = state.get("testcases_document")
    uploaded_files = state.get("uploaded_files", [])
    retry_count = state.get("retry_count", {})
    
    try:
        if execution_mode in ["testcases_only", "parallel"]:
            document = testcases_doc
            if not document and uploaded_files:
                first_file = uploaded_files[0]
                document = first_file.get("content") or first_file.get("path") or first_file.get("name")
            
            if not document:
                return {
                    **state,
                    "error": "No document available for test case generation",
                    "messages": [AIMessage("âŒ Cannot generate test cases: No document found.")],
                }
            
            print(f"ðŸ§ª Generating test cases directly from document: {document}")
            result = test_case_generation_tool(
                {"document": document, "name": "uploaded_document"},
                {"execution_mode": "independent"}
            )
        else:
            if not requirements:
                return {
                    **state,
                    "error": "No requirements available for test case generation",
                    "messages": [AIMessage("âŒ Cannot generate test cases: No requirements found.")],
                }
            
            print(f"ðŸ§ª Generating test cases from existing requirements...")
            result = test_case_generation_tool(requirements)
        
        retry_count["testcases"] = 0
        
        return {
            **state,
            "testcases_output": result,
            "messages": [AIMessage("âœ… Test cases generated.")],
            "retry_count": retry_count,
            "error": None
        }
    except Exception as e:
        retry_count["testcases"] = retry_count.get("testcases", 0) + 1
        return {
            **state,
            "error": f"Test case generation failed after retries: {str(e)}",
            "messages": [AIMessage(f"âŒ Error: {str(e)}")],
            "retry_count": retry_count
        }


# ============================================================================
# ENHANCED HUMAN-IN-THE-LOOP NODES
# ============================================================================

def human_approval_prompt_node(state: AgentState) -> AgentState:
    """
    Enhanced approval prompt with better formatting.
    """
    reqs = state["requirements_output"]

    message = AIMessage(
        content=f"""
ðŸ“‹ **REQUIREMENTS GENERATED â€“ APPROVAL REQUIRED**

âœ… Generation completed successfully!

Reply with:
- **approve** / **yes** / **y** â†’ continue to test case generation
- **reject** / **no** / **n** â†’ stop the process

Your response:
"""
    )

    return {
        **state,
        "messages": [message],
        "human_approval_required": True,
        "human_approved": None,
    }


def human_approval_execute_node(state: AgentState) -> AgentState:
    """
    Enhanced approval execution with better validation.
    """
    feedback = (state.get("human_feedback") or "").lower().strip()
    
    reject_keywords = {"reject", "no", "n", "deny", "stop", "cancel"}
    approve_keywords = {"approve", "yes", "y", "accept", "continue", "ok", "proceed"}
    
    if any(kw in feedback for kw in reject_keywords):
        return {
            **state,
            "human_approved": False,
            "process_terminated": True,
            "messages": [
                AIMessage("âŒ Requirements rejected. Process terminated.")
            ],
        }
    elif any(kw in feedback for kw in approve_keywords):
        return {
            **state,
            "human_approved": True,
            "human_approval_required": False,
            "messages": [
                AIMessage("âœ… Requirements approved. Proceeding to test case generation...")
            ],
        }
    else:
        return {
            **state,
            "messages": [
                AIMessage("âš ï¸ Unclear response. Please respond with 'approve' or 'reject'")
            ],
        }


# ============================================================================
# ENHANCED PARALLEL EXECUTION
# ============================================================================

def parallel_execution_node(state: AgentState) -> AgentState:
    """
    Enhanced parallel execution with better timeout handling.
    """
    requirements_doc = state.get("requirements_document")
    testcases_doc = state.get("testcases_document")
    uploaded_files = state.get("uploaded_files", [])
    retry_count = state.get("retry_count", {})
    
    if not requirements_doc and len(uploaded_files) >= 1:
        first_file = uploaded_files[0]
        requirements_doc = first_file.get("content") or first_file.get("path") or first_file.get("name")
    
    if not testcases_doc and len(uploaded_files) >= 2:
        second_file = uploaded_files[1]
        testcases_doc = second_file.get("content") or second_file.get("path") or second_file.get("name")
    
    print(f"ðŸ”„ Parallel execution started")
    print(f"   Requirements doc: {requirements_doc}")
    print(f"   Test cases doc: {testcases_doc}")
    
    requirements_result = None
    testcases_result = None
    errors = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        futures = {}
        
        if requirements_doc:
            futures["requirements"] = executor.submit(
                requirements_creation_tool, requirements_doc, {}
            )
        
        if testcases_doc:
            futures["testcases"] = executor.submit(
                test_case_generation_tool,
                {"document": testcases_doc, "name": "testcases_document"},
                {"execution_mode": "independent"}
            )
        
        for task_name, future in futures.items():
            try:
                result = future.result(timeout=600)
                if task_name == "requirements":
                    requirements_result = result
                    retry_count["requirements"] = 0
                    print("âœ… Requirements generation completed")
                else:
                    testcases_result = result
                    retry_count["testcases"] = 0
                    print("âœ… Test cases generation completed")
            except concurrent.futures.TimeoutError:
                error_msg = f"{task_name} timed out after 10 minutes"
                errors.append(error_msg)
                retry_count[task_name] = retry_count.get(task_name, 0) + 1
                print(f"â±ï¸ {error_msg}")
            except Exception as e:
                error_msg = f"{task_name} failed: {str(e)}"
                errors.append(error_msg)
                retry_count[task_name] = retry_count.get(task_name, 0) + 1
                print(f"âŒ {error_msg}")
    
    return {
        **state,
        "requirements_output": requirements_result,
        "testcases_output": testcases_result,
        "error": "; ".join(errors) if errors else None,
        "retry_count": retry_count,
        "messages": [AIMessage("âœ… Parallel execution completed." if not errors else f"âš ï¸ Completed with errors: {'; '.join(errors)}")],
    }


# ============================================================================
# ERROR HANDLER NODE
# ============================================================================

def error_handler_node(state: AgentState) -> AgentState:
    """
    Enhanced error handling with recovery options.
    """
    error = state.get("error", "Unknown error occurred")
    retry_count = state.get("retry_count", {})
    
    if "timeout" in error.lower():
        message = f"""â±ï¸ **TIMEOUT ERROR**

The process timed out. This can happen with large documents or slow network.

Retry attempts: {retry_count}

Would you like to:
1. Retry the operation
2. Try with a smaller document
3. End the process

Reply with: retry / smaller / end
"""
    elif "api" in error.lower() or "connection" in error.lower():
        message = f"""ðŸ”Œ **API CONNECTION ERROR**

Cannot connect to the API endpoint.

Retry attempts: {retry_count}

Please check:
- Network connection
- API endpoint status
- Firewall settings

Reply with: retry / end
"""
    else:
        message = f"""âŒ **ERROR OCCURRED**

{error}

Retry attempts: {retry_count}

Reply with: retry / end
"""
    
    return {
        **state,
        "messages": [AIMessage(message)],
    }


# ============================================================================
# ROUTING FUNCTIONS
# ============================================================================

def route_from_agent(state: AgentState) -> Literal[
    "requirements", "human_approval_prompt", "testcases", "parallel_execution", "error_handler", "end"
]:
    """Enhanced routing with error handling."""
    execution_mode = state.get("execution_mode", "sequential")
    requirements_output = state.get("requirements_output")
    testcases_output = state.get("testcases_output")
    human_approved = state.get("human_approved")
    human_approval_required = state.get("human_approval_required")
    error = state.get("error")
    
    if error:
        return "error_handler"
    
    if execution_mode == "testcases_only":
        if not testcases_output:
            print("ðŸ“‹ Testcases-only mode: Generating test cases from document")
            return "testcases"
        return "end"
    
    if execution_mode == "requirements_only":
        if not requirements_output:
            return "requirements"
        return "end"
    
    if execution_mode == "parallel":
        if not requirements_output and not testcases_output:
            print("ðŸ“‹ Parallel mode: Generating requirements and test cases simultaneously")
            return "parallel_execution"
        return "end"
    
    if not requirements_output:
        return "requirements"

    if human_approval_required and human_approved is None:
        return "human_approval_prompt"

    if human_approved and not testcases_output:
        print("ðŸ“‹ Using existing requirements for test case generation")
        return "testcases"
    
    if requirements_output and testcases_output:
        return "end"

    return "end"


def route_after_approval(state: AgentState) -> Literal["agent", "end"]:
    """Enhanced routing after approval."""
    if state.get("process_terminated"):
        return "end"
    if state.get("error"):
        return "end"
    return "agent"


def route_from_error(state: AgentState) -> Literal["agent", "end"]:
    """Route from error handler based on user input."""
    feedback = (state.get("human_feedback") or "").lower().strip()
    
    if "retry" in feedback:
        return "agent"
    
    return "end"


# ============================================================================
# GRAPH CONSTRUCTION
# ============================================================================

_shared_memory = MemorySaver()


def create_agent_graph():
    """Create the enhanced agent graph."""
    workflow = StateGraph(AgentState)

    workflow.add_node("agent", agent_planner)
    workflow.add_node("requirements", requirements_node)
    workflow.add_node("human_approval_prompt", human_approval_prompt_node)
    workflow.add_node("human_approval_execute", human_approval_execute_node)
    workflow.add_node("testcases", testcases_node)
    workflow.add_node("parallel_execution", parallel_execution_node)
    workflow.add_node("error_handler", error_handler_node)

    workflow.add_edge(START, "agent")

    workflow.add_conditional_edges(
        "agent",
        route_from_agent,
        {
            "requirements": "requirements",
            "human_approval_prompt": "human_approval_prompt",
            "testcases": "testcases",
            "parallel_execution": "parallel_execution",
            "error_handler": "error_handler",
            "end": END,
        },
    )

    workflow.add_edge("requirements", "agent")
    workflow.add_edge("human_approval_prompt", "human_approval_execute")
    workflow.add_edge("parallel_execution", END)

    workflow.add_conditional_edges(
        "human_approval_execute",
        route_after_approval,
        {
            "agent": "agent",
            "end": END,
        },
    )

    workflow.add_edge("testcases", END)
    
    workflow.add_conditional_edges(
        "error_handler",
        route_from_error,
        {
            "agent": "agent",
            "end": END,
        },
    )

    return workflow.compile(
        checkpointer=_shared_memory,
        interrupt_before=["human_approval_execute"],
    )


# ============================================================================
# RUN / RESUME FUNCTIONS WITH AUTO INTENT DETECTION
# ============================================================================

def run_agent(user_query: str, uploaded_files: list = None, execution_mode: str = None, use_llm_inference: bool = True):
    """
    Enhanced run function with automatic execution mode inference.
    
    Args:
        user_query: User's query
        uploaded_files: List of uploaded files
        execution_mode: Optional override for execution mode
        use_llm_inference: Use LLM for intent detection (default: True, fallback to rule-based)
    
    Returns:
        session_id: Session ID for resuming
    """
    graph = create_agent_graph()
    session_id = str(uuid.uuid4())
    
    # AUTO-INFER execution mode if not provided
    if execution_mode is None:
        if use_llm_inference:
            try:
                llm_result = llm_infer_execution_mode(user_query, uploaded_files)
                execution_mode = llm_result.get("execution_mode")
                print(f"âœ… Using LLM-inferred execution mode: {execution_mode}")
            except Exception as e:
                print(f"âš ï¸ LLM inference failed: {e}")
                execution_mode = infer_execution_mode(user_query, uploaded_files)
                print(f"âœ… Using rule-based execution mode: {execution_mode}")
        else:
            execution_mode = infer_execution_mode(user_query, uploaded_files)
            print(f"âœ… Using rule-based execution mode: {execution_mode}")
    else:
        print(f"â„¹ï¸ Using user-provided execution mode: {execution_mode}")
    
    # Generate intent cache key
    files_hash = hashlib.md5(
        json.dumps([f.get("name", "") for f in (uploaded_files or [])]).encode()
    ).hexdigest()
    intent_cache_key = f"{user_query}_{files_hash}"

    initial_state: AgentState = {
        "messages": [HumanMessage(content=user_query)],
        "requirements_output": None,
        "testcases_output": None,
        "human_approval_required": False,
        "human_approved": None,
        "human_feedback": None,
        "process_terminated": False,
        "error": None,
        "iteration_count": 0,
        "max_iterations": 5,
        "session_id": session_id,
        "timestamp": datetime.now().isoformat(),
        "uploaded_files": uploaded_files or [],
        "execution_mode": execution_mode,
        "requirements_document": None,
        "testcases_document": None,
        "progress": None,
        "intent_cache_key": intent_cache_key,
        "retry_count": {}
    }

    config = {"configurable": {"thread_id": session_id}}

    print(f"\n{'='*80}")
    print(f"SESSION STARTED: {session_id}")
    print(f"Execution Mode: {execution_mode}")
    print(f"{'='*80}\n")

    for event in graph.stream(initial_state, config):
        for node_name, node_state in event.items():
            print(f"\n[{node_name.upper()}]")
            if isinstance(node_state, dict):
                if node_state.get("messages"):
                    print(node_state["messages"][-1].content)
                if node_state.get("progress"):
                    progress = node_state["progress"]
                    print(f"\nðŸ“Š Progress:")
                    print(f"   - Requirements: {'âœ“' if progress.get('requirements_generated') else 'â³'}")
                    print(f"   - Approval: {'âœ“' if progress.get('requirements_approved') else 'â³'}")
                    print(f"   - Test Cases: {'âœ“' if progress.get('testcases_generated') else 'â³'}")
                    print(f"   - Current Step: {progress.get('current_step', 'N/A')}")

    print(f"\n{'='*80}")
    print(f"SESSION ID: {session_id}")
    print(f"{'='*80}\n")
    
    return session_id


def resume_agent_with_approval(session_id: str, approval_response: str):
    """
    Enhanced resume function with better state management.
    """
    graph = create_agent_graph()
    config = {"configurable": {"thread_id": session_id}}

    snapshot = graph.get_state(config)
    
    if not snapshot or not snapshot.values:
        print("âŒ No state found for this session.")
        return
    
    state = dict(snapshot.values)
    state["human_feedback"] = approval_response
    
    if "messages" not in state:
        state["messages"] = []
    state["messages"].append(HumanMessage(content=approval_response))

    graph.update_state(config, state)

    print(f"\n{'='*80}")
    print(f"RESUMING SESSION: {session_id}")
    print(f"{'='*80}\n")

    for event in graph.stream(None, config):
        for node_name, node_state in event.items():
            print(f"\n[{node_name.upper()}]")
            if isinstance(node_state, dict) and node_state.get("messages"):
                print(node_state["messages"][-1].content)


def get_session_progress(session_id: str) -> dict:
    """
    Get current progress of a session.
    """
    graph = create_agent_graph()
    config = {"configurable": {"thread_id": session_id}}
    
    snapshot = graph.get_state(config)
    if not snapshot or not snapshot.values:
        return {"error": "Session not found"}
    
    state = snapshot.values
    return {
        "progress": state.get("progress"),
        "retry_count": state.get("retry_count"),
        "error": state.get("error"),
        "execution_mode": state.get("execution_mode")
    }


# ============================================================================
# DEMO / MAIN
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*80)
    print("ENHANCED LANGGRAPH v1.0 WITH AUTO INTENT DETECTION")
    print("="*80)
    print("\nâœ¨ Key Features:")
    print("  âœ“ Automatic execution mode inference (LLM + rule-based)")
    print("  âœ“ No need to specify execution_mode manually")
    print("  âœ“ Automatic retry with exponential backoff")
    print("  âœ“ Progress tracking")
    print("  âœ“ Enhanced error handling and recovery")
    print("  âœ“ Better human approval validation")
    print("  âœ“ Timeout handling for parallel execution")
    print("="*80 + "\n")
    
    # Example documents
    doc1 = {
        "name": "ts.pdf",
        "type": "pdf",
        "content": r"c:\Users\dasan\OneDrive - Broadridge Financial Solutions, Inc\Projects\DeepAgents\Backend\ts.pdf"
    }
    
    # ========================================================================
    # EXAMPLE 1: Auto-detect "requirements only"
    # ========================================================================
    print("\n" + "="*80)
    print("EXAMPLE 1: Auto-detect Requirements Only")
    print("="*80)
    query1 = "Generate requirements from this document"
    session_id1 = run_agent(query1, uploaded_files=[doc1])
    
    # ========================================================================
    # EXAMPLE 2: Auto-detect "test cases only"
    # ========================================================================
    print("\n\n" + "="*80)
    print("EXAMPLE 2: Auto-detect Test Cases Only")
    print("="*80)
    query2 = "Create test cases from this document"
    # Uncomment to run:
    # session_id2 = run_agent(query2, uploaded_files=[doc1])
    
    # ========================================================================
    # EXAMPLE 3: Auto-detect "sequential" (both from one file)
    # ========================================================================
    print("\n\n" + "="*80)
    print("EXAMPLE 3: Auto-detect Sequential (Both from one file)")
    print("="*80)
    query3 = "Generate both requirements and test cases from this document"
    # Uncomment to run:
    # session_id3 = run_agent(query3, uploaded_files=[doc1])
    
    # ========================================================================
    # EXAMPLE 4: Auto-detect "parallel" (two files)
    # ========================================================================
    print("\n\n" + "="*80)
    print("EXAMPLE 4: Auto-detect Parallel (Two files)")
    print("="*80)
    doc2 = {
        "name": "test_scenarios.pdf",
        "type": "pdf",
        "content": r"c:\path\to\test_scenarios.pdf"
    }
    query4 = "Generate requirements from first document and test cases from second document in parallel"
    # Uncomment to run:
    # session_id4 = run_agent(query4, uploaded_files=[doc1, doc2])
    
    # ========================================================================
    # EXAMPLE 5: Manual override (advanced use)
    # ========================================================================
    print("\n\n" + "="*80)
    print("EXAMPLE 5: Manual Override (Advanced)")
    print("="*80)
    query5 = "Process this document"  # Ambiguous query
    # Force parallel mode despite ambiguous query:
    # session_id5 = run_agent(query5, uploaded_files=[doc1, doc2], execution_mode="parallel")
    
    print("\n\n" + "="*80)
    print("All examples demonstrated! Uncomment to run additional examples.")
    print("="*80)
